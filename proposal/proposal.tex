\documentclass[11pt,twocolumn]{article}

\usepackage{hyperref}
% \usepackage{fullpage}

\linespread{1.2}

\begin{document}

\title{CSE 549\\Computational Biology -- Project Proposal\\Genome Compression}
\author{Pragya Pande \and Dhruv Matani}
\date{\today}

\maketitle

\vspace{0.5in}

\section*{The problem}
The Complete Human Genome was sequenced in 2003. Since then a lot of
research is being done in genomics and computational biology. The
major input for most of the computation is the 2.9 billion base pairs
\cite{2}\cite{3} of the human genome which correspond to a maximum of about 
725 megabytes of data human genome.\cite{4}

Furthermore, reduction in the cost of sequencing(via the “next-gen” 
sequencing platforms),has given birth to the 1000 genome project
(\url{http://www.1000genomes.org/}) which aims to sequence the genomes of a large 
number of people. Just like the other human genome reference projects,
this data (estimated 8.2 billion bases per day) would be made
available via public databases for the scientific community.\cite{5}

As we can now see, we are dealing with megabytes and megabytes of data 
when we work with genomes! This gives rise to challenging 
problems with respect to storage, distribution (downloading, copying), 
and sharing of this genomic data. Hence we need to consider better 
compression techniques for the genomic data.

In this project, we explore this problem of genome compression and see
if we can compress the human genome to more than what the previous
researcher have done.\cite{6}

\section*{Applicability}

The goal of the 1000 genomes project\footnote{\url{http://www.1000genomes.org/}}
is to find most genetic variants that have frequencies of at least 1\%
in the populations studied. Similarly, once the \$1000 genome
project\cite{1000genomeproject} becomes successful, storage costs for
all the sequenced genomes will need to be kept under control. To be
able to do this, we need an space (and time) efficient way of
compressing the sequenced genomes so that the DNA of more people can
be sequenced for a reasonable price.



\section*{Current work}

A group of researchers from The University of California at Irvine
lead by Dr. Chen Li\footnote{http://www.ics.uci.edu/\textasciitilde{}chenli/} have
researched the compressibility of the human genome against a reference
genome and have achieved fascinating results\cite{dnazip}. They have been able to
compress James Watson's genome\footnote{James Watson's genome is a
  418MiB compressed downloadable tarball} which is 3 billion base
pairs long to about 4MB of compressed data. This is small enough to be
emailed across the globe!

They have used the repititive nature of the genomic data to their
advantage to aid compression. Also, since the genomes of most humans
differs in approximately 1 base pair in about 1000, there are a lot of
gains to be had by compressing a human genome it against a reference
human genome.

Their technique relies on a few key
observations\cite{genomecompressionchenli}:

\begin{enumerate}

\item \textbf{Variable integers for positions (VINT)}: Chromosome
  sizes can vary. Some can be up to 250MiB in size. Representing
  offsets within these files would require 4 bytes. However, many
  smaller offsets don't need such large integers to store the
  data. Variable size integers are helpful in such situations.

\item \textbf{Delta positions (DELTA)}: The idea of storing delta
  offsets v/s absolute offsets seems to be one borrowed from
  run-length encoding of strings where repetitive patterns are stored
  as offsets relative to where they last occured.

\end{enumerate}

\subsection*{}

\section*{The plan}

\subsection*{Understanding Prior Work}

We need to refer to the papers and links in the \textit{References}
section and understand the ideas presented therein so that we can
potentially incorporate or improve upon them in our project.

\subsection*{Getting the data}

We plan on using the same data that the previous research group from
UC Irvine has used so that we can compare the compression achieved by
our technique against theirs.

The various data sets used by the researchers are:

\begin{enumerate}

\item The human reference genome\footnote{Compressed size is
  840MiB}. This is used as the reference genome against which
  differences are compared. There are 25 FASTA files, one for each
  chromosome and the mitochondrial genome.

\item dbSNP database, used as the refernece SNP\footnote{A
  Single-nucleotide polymorphism is a DNA sequence variation occurring
  when a single nucleotide - A, T, C or G - in the genome (or other
  shared sequence) differs between members of a biological species or
  paired chromosomes in an individual} map.

\item James Watson's genome\footnote{Compressed size is 418MiB}, used
  as the example genome to be compressed.

\end{enumerate}


\subsection*{Handling FASTA files}
As mentioned in the above section, we have 25 FASTA files as 
our input, sowe need to parse before we begin using them. 
For this purpose, we are investigating the following FASTA tools :
\begin{enumerate}
\item FASTA/FASTQ Parser in C\cite{7}
\item UVa FASTA Downloads\cite{8}
\end{enumerate}

We will be choosing the tools which would work well on our huge 
DNA sequences with the least good memory footprint. 

\subsection*{Rough estimates of compressibility}

The human genome is about 3 billion base-pairs long. If each base is
represented as a single (8-bit) ASCII character, then we need about
3GB of storage space to store the human genome. However, since each
base can hold one one of 4 bases, we can encode each base as a 2-bit
word. This immediately reduces our storage requirement by 75\% and
results in a 75\% compression, bringing down the storage to 750 MB.

Simple gzip compression also achieves similar compressibility.

The researchers were able to generate a \textit{diff} of size 84MB
(uncompressed) of the human genome against the reference genome. This
was further compressed down to about 4MB. This is a compression of
about 95\%. Such compression may be hard to achieve if the specific
constraints of the problem under consideration are not exploited.

We think that we should be able to do better than 84MB while
generating the difference between genomes and we plan to concentrate
more of our energies there.

\clearpage

\begin{thebibliography}{9}

\bibitem{dnazip}
DNAzip: DNA sequence compression using a reference genome
\url{http://www.ics.uci.edu/~dnazip/}
\bibitem{2}
http://www.strategicgenomics.com/Genome/index.htm
\bibitem{3}
http://www.nature.com/nature/journal/v431/n7011/abs/nature03001.html
\bibitem{4}
http://en.wikipedia.org/wiki/Human\_genome
\bibitem{5}
http://www.1000genomes.org/about
\bibitem{6}
http://bioinformatics.oxfordjournals.org/content/25/2/274.full
\bibitem{7}
http://lh3lh3.users.sourceforge.net/parsefastq.shtml
\bibitem{8}
http://fasta.bioch.virginia.edu/fasta\_www2/fasta\_down.shtml

\bibitem{1000genomeproject}
Anticipating the 1,000 dollar genome.
Mardis ER

\bibitem{genomecompressionchenli}
Human genomes as email attachments.
Scott Christley, Yiming Lu, Chen Li, \and Xiaohui Xie


\end{thebibliography}



\end{document}
