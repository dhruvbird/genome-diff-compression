\documentclass[11pt]{article}

\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{color}
% \usepackage{fullpage}

\linespread{1.2}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1.9ex plus 0.5ex minus 0.2ex}

\begin{document}

\title{Compressing the Human Genome against a reference}
\author{Pragya Pande\\\texttt{\small{ppande@cs.stonybrook.edu}}\\\small{Stony Brook University} \and 
  Dhruv Matani\\\texttt{\small{dmatani@cs.stonybrook.edu}}\\\small{Stony Brook University}
}
\date{\today}

\maketitle

\vspace{0.5in}

\begin{abstract}
  We present a way to compress the complete human genome against a
  reference genome to achieve \textit{139x} compression. Our
  approaches compresses a \textit{2.6GiB} human genome to about
  \textit{19.2MiB}. The compression method is such that decompression
  from random offsets is fast. Our method does not rely on the target
  genome being almost of the same size as the reference genome, or the
  existence of any alignment program available to us. We shall also
  prove that for the representation used, the technique we propose
  achieves optimal compression.

  We shall first motivate the problem itself, show existing research
  that has been done in this direction, present our approach, and
  compare it with prior work.

\end{abstract}

\clearpage

\section{Motivation}

The Complete Human Genome was sequenced in 2003. Since then a lot of
research is being done in genomics and computational biology. The
major input for most of the computation is the 2.9 billion base pairs
\cite{howmuchsequenced}\cite{findinghumangenome} of the human genome
which correspond to a maximum of about 725 megabytes of human
genome data.\cite{wikipediahumangenome}

Furthermore, reduction in the cost of sequencing (via the ``next-gen''
sequencing platforms) has given birth to the 1000 genomes project
\footnote{\url{http://www.1000genomes.org/}} which aims to sequence
the genomes of a large number of people. Just like the other human
genome reference projects, this data (estimated 8.2 billion bases per
day) would be made available via public databases for the scientific
community.\cite{1000genomes}

As we can now see, we are dealing with megabytes and megabytes of data 
when we work with genomes! This gives rise to challenging 
problems with respect to storage, distribution (downloading, copying), 
and sharing of this genomic data. Hence we need to consider better 
compression techniques for the genomic data. That apart, when working
with genomic data, we want to be able to optimize decompression so
that working with these compressed genomes is no harder than working
with the uncompressed genome.

The goal of the 1000 genomes project is to find most genetic variants
that have frequencies of at least 1\% in the populations
studied. Similarly, once the \$1000 genome
project\cite{1000dollargenomeproject} becomes successful, storage
costs for all the sequenced genomes will need to be kept under
control. To be able to do this, we need an space (and time) efficient
way of compressing the sequenced genomes so that the DNA of more
people can be sequenced for a reasonable price.

We explore the problem of genome compression and see if we can:

\begin{itemize}

\item Better align 2 human genomes so as to facilitate better
  compression on them

\item Better compress the human genome to a smaller on-disk
  representation

\item Enable faster decompression

\item Enable space-efficient decompression

\item Enable I/O efficient decompression so that working with the
  compressed genome is the norm rather than the exception

\end{itemize}

\section{Existing Research}

\subsection{gzip \& bzip2}

These 2 applications are general lossless compression routines that
use run-length encoding and block sort compression respectively. They
can compress the human genome (about 3GiB) to about 830MiB, which is a
compression ratio of \textit{3.67}.

\subsection{GenCompress, BioCompress \& Cfact}

GenCompress\cite{gencompress}, BioCompress\cite{biocompress} \&
Cfact\cite{cfact} are tools that mostly rely on either Huffman Coding,
Ziv-Lempel\cite{zivlempel}, or Arithmetic Coding to compress the human
genome. They are able achieve better compression when compared with
\textit{gzip \& bzip2}, but not as much as some of the difference
encoding methods mentioned below. These methods achieve compression
ratios of anywhere from \textit{4.82 -- 7.00}.

\subsection{Difference Encoding Techniques}

Difference encoding schemes are getting very popular since they can
achieve very high compression ratios of greater than \textit{100}
since \textit{99.9\%} of the genomes of 2 humans are similar to each
other. There are very few variations between the genomes of 2
individuals.

DNAzip\cite{dnazip} was the first algorithm to compress the target
genome by storing differences between the target and reference
genome. However, DNAzip does not solve the problem of aligning 2
genomes with each other, but instead assumes the existence of an
\textit{SNP -- Single Nucleotide Polymorphism} file to exist, which it
takes in as input. In our experience, aligning 2 genomes is a very
time consuming process and is orders of magnitude slower than
performing the actual compression itself.

Wang \& Zhang\cite{wangzhang} have come up with a difference
compression technique that is able to compress \textit{2986MiB} of a
Korean genome to \textit{18.8MiB}. This was done by difference
compressing the target genome it against another Korean genome which
was sequenced using similar methods and had the about the same
size. their technique considers blocks of bases of size 50, 20 or 10
million, and groups each chromosome from both the target and the
reference in these blocks to compute the difference between them. This
may not always work with genomes that have sequenced and assembled
using different techniques, or if you are using the human genome as a
reference to compress the mouse genome for example.

Researchers at the University of Virginia\cite{vtechresearch} have
also come up with a difference compression algorithm that focuses on
decompression speed rather than raw compression ratio. We are also
motivated by some of the same factors that have motivated their
research. The technique presented achieves decompression times of
$O(\log{n})$ per random offset query. We also target a similar
complexity for random offset queries. They haven't publish exact
numbers on compressing the human genome, but claim that they achieve
\textit{98.8\%} compression when compressing mitochondrial sequences,
which roughly compresses the human genome of \textit{3GiB} to
\textit{36MiB}. Their technique also relies on align sequences of
chromosomes in the target with those in the reference genome. They use
the multiple sequence alignment application MUSCLE\cite{muscle} to
align the sequences with the reference genome.

Researchers at Cairo University, Egypt\cite{cairo} use existing
local-sequence-alignment programs to align the target and reference
sequence with each other. They then record insertions, deletions and
modifications with respect to the reference sequence and encode these
differences in a compact fashion. They have achieved \textit{99.4\%}
compression on the human genome, which means that they can compress
\textit{3GiB} of genomic sequence data to about
\textit{18.5MiB}. However, they don't talk much about decompression,
and it seems as if it is not entirely easy to decompress to random
offsets in the target genome. i.e. Ask for a sequence of length
\textit{l} at offset \textit{k} from the uncompressed genome.

\section{Our Solution}

\subsection{Outline of the Approach}

We compress the genome a chromosome at a time. $Ref_i$ is our
reference chromosome and $Victim_i$ is the chromosome to be
compressed.

We shall find the optimal (least) number of ranges from $Ref_i$ that
can completely cover $Victim_i$. To do this, we shall construct a
Suffix Array on $Ref_i$ \& $Victim_i$ and then find the least range
covering. We shall prove that for an encoding that only encodes
\textit{(offset, length)} pairs from the reference, this
representation is optimal in terms of the number of ranges needed to
completely cover the entire target chromosome.

\subsection{The Algorithm}

\begin{enumerate}

\item For each corresponding chromosome pair ${Ref}_i$ \& $Victim_i i
  \in [1\ldots{}23]$, build a
  \textit{suffix array}\footnote{This is done using Manber \& Myers
    $O(n\log{n})$ suffix array construction
    algorithm\cite{manbermyers}, which allows us to compute the
    \textit{Longest common Prefix} of 2 adjacent suffixes in
    $O(\log{n})$ time} from the string $Ref_i\#Victim_i\$$.

\item For every suffix in ${Victim}_i$, find the length of the longest
  prefix that matches with some string at index $j$ in ${Ref}_i$.

\item Now that we have all the ranges in $Ref_i$ that $Victim_i$ can
  be covered by, we find the least number of ranges that can
  completely cover $Victim_i$. This can be done in $O(n\log{n})$ time
  using \textit{Dynamic Programming} and \textit{Segment Trees} as an
  online \textit{Range Minimum Query} data structure. The problem of
  finding the minimal number of completely covering sub-ranges
  exhibits optimal substructure. i.e. A solution for the range
  $[i\ldots{}n]$ can be constructed using the solution to the ranges
  $[i+1\ldots{}n], [i+2\ldots{}n], [i+3\ldots{}n], \ldots{},
  [i+k-1\ldots{}n]$, where $k$ is the length of the range starting at
  index $i$.

\end{enumerate}

\subsection{Proof of Optimality}

We shall prove by contradiction that for the range based encoding
scheme we have used, the algorithm presented above achieves optimal
compression in terms of the number of ranges used to represent the
completely covered \textit{victim} chromosome.

Given the following range lengths starting at the indexes at which
they occur, what is the least number of ranges that completely cover
the whole range?

\begin{center}
  \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
    \hline
    Index        & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 &10 &11 &12 &13\\
    \hline
    Range Length & 3 & 2 & 4 & 1 & 1 & 1 & 3 & 6 & 8 & 2 & 1 & 2 & 1 & 1\\
    \hline
  \end{tabular}\\
  \vspace{0.3cm}
  Indexes and lengths of ranges at those indexes
\end{center}

The best solution extends the smallest range that it can cover to its
right.

\begin{center}
  \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
    \hline
    Index         & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 &10 &11 &12 &13\\
    \hline
    Range Length  & 3 & 2 & 4 & 1 & 1 & 1 & 3 & 6 & 8 & 2 & 1 & 2 & 1 & 1\\
    \hline
    Min n(Ranges) & 4 & 4 & 3 & 5 & 4 & 3 & 2 & 2 & 1 & 3 & 3 & 2 & 2 & 1\\
    \hline
    Next Index    & \textcolor{red}{3} & 2 & 6 & \textcolor{red}{4} & \textcolor{red}{5} & \textcolor{red}{6} & \textcolor{red}{8} & 8 & \textcolor{red}{14} &10 &11 &13 &13 &14\\
    \hline
  \end{tabular}\\
  \vspace{0.3cm}
  The numbers in \textcolor{red}{red} indicate the range end-points
  that make up the ranges that cover the entire sequence. If there are
  \textit{n} numbers, there exist \textit{(n-1)} ranges. Range
  extension stops when we have gone past the last index in the
  sequence (14 in this example).
\end{center}

If there is a better solution at any stage, then it would have been
the one chosen by our algorithm for extending a range from a given
index.


\section{Approaches Compared}

\subsection{DNAzip}

The DNAzip group relies on an existing \textit{SNP} file being
present, and hence they don't do sequence alignment before compressing
the difference file.

It isn't possible to get the decompressed genome data at a certain
\textit{(offset, length)} pair without decompressing the complete
genome.

\subsection{Wang \& Zhang's method}

Wang \& Zhang's method is called \textit{The GRS tool} and it works in
blocks of size 50, 25, or 10 million. It relies on being able to
compute the longest common sequence between 2 blocks. It isn't
immediately apparent how easy it would be to support fast \& efficient
random offset querying on the compressed data generated by \textit{The
  GRS tool}.

\section{Experimental Results}

\section{Future Work}

\section{Acknowledgments}

We would like to thank Dr. Skiena for giving us the opportinity to
work on this project and rightly penalizing us for a less-than-optimal
project proposal \& progress report. This made us work harder on the
project. Though, we would like to believe that the real reason for our
earlier lethargy was the phenomenon of \textit{Temporal
  Discounting}\footnote{\url{http://en.wikipedia.org/wiki/Temporal_discounting}:
  Temporal discounting refers to the tendency of people to discount
  rewards as they approach a temporal horizon in the future or the
  past (i.e., become so distant in time that they cease to be valuable
  or to have additive effects).}

We would also like to thank Aniruddha Laud, Gaurav menghani, \&
Madhava K R, who patiently explained the working of
DNAzip\cite{dnazip} to us.



\clearpage

\begin{thebibliography}{9}

\bibitem{dnazip} DNAzip: DNA sequence compression using a reference
  genome \url{http://www.ics.uci.edu/~dnazip/}
\bibitem{howmuchsequenced}
How much of the human genome has been sequenced?
\url{http://www.strategicgenomics.com/Genome/index.htm}
\bibitem{findinghumangenome}
Finishing the euchromatic sequence of the human genome
\url{http://www.nature.com/nature/journal/v431/n7011/abs/nature03001.html}
\bibitem{wikipediahumangenome}
Wikipedia on The Human Genome
\url{http://en.wikipedia.org/wiki/Human\_genome}
\bibitem{1000genomes}
The 100 genomes project aims to provide a comprehensive resource on
human genetic variation. \url{http://www.1000genomes.org/about}
\bibitem{zivlempel}
A Universal Algorithm for Sequential Data Compression, IEEE
Trans. Information Theory, vol. 23, pp. 337-343, 1977.
\bibitem{gencompress}
DNABIT Compress -- Genome compression algorithm. Pothuraju
Rajarajeswari and Allam Apparao
\bibitem{biocompress}
A New Challenge for Compression Algorithms: Genetic Sequences,
S. Grumbach and F. Tahi, Information Processing Management, vol. 30,
no. 6, pp. 875-886, 1994.
\bibitem{cfact}
A Guarantee DNA Sequences for Repetitive DNA Sequences, E. Rivals,
J.P.Delahaye, M.Dauchet, and O.Delgrange, LIFL Lille I University,
Technical Report IT-285, 1995.
\bibitem{1000dollargenomeproject}
Anticipating the 1,000 dollar genome, Mardis ER
\bibitem{wangzhang}
A novel compression tool for efficient storage of genome resequencing
data. Congmao Wang and Dabing Zhang
\bibitem{vtechresearch}
A Genome Compression Algorithm Supporting Manipulation. Lenwood
S. Heath, Ao-ping Hou, Huadong Xia, and Liqing Zhang
\bibitem{muscle}
MUSCLE: Multiple sequence alignment with high accuracy and high
throughput, R. C. Edgar, Nucleic Acids Research, 32 (2004),
pp. 1792â€“1797.
\bibitem{cairo}
DNA Lossless Difference Compression Algorithm Based On Similarity Of
Genomic Sequence Database, Heba Afify, Muhammad Islam, and Manal Abdel
Wahed
\bibitem{manbermyers}
Suffix arrays: A new method for on-line string searches, Udi Manber
and Gene Myers
\bibitem{6}
http://bioinformatics.oxfordjournals.org/content/25/2/274.full
\bibitem{7}
http://lh3lh3.users.sourceforge.net/parsefastq.shtml
\bibitem{8}
http://fasta.bioch.virginia.edu/fasta\_www2/fasta\_down.shtml


\bibitem{genomecompressionchenli}
Human genomes as email attachments.
Scott Christley, Yiming Lu, Chen Li, \and Xiaohui Xie


\end{thebibliography}

\end{document}
